# main.py (EM-LLMçµ±åˆç‰ˆ)
"""
EM-LLMå¯¾å¿œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒªã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

ä¸»ãªå¤‰æ›´ç‚¹:
1. EM-LLMçµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆæœŸåŒ–
2. æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½¿ç”¨
3. EM-LLMçµ±è¨ˆã®å‡ºåŠ›
4. æ®µéšçš„ç§»è¡Œã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½

æµã‚Œ:
1) LLMãƒ­ãƒ¼ãƒ‰
2) EM-LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
3) äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼çµŒç”±ã§LangGraphã‚¢ãƒ—ãƒªæ§‹ç¯‰
4) EM-LLMæ©Ÿèƒ½ä»˜ãå¯¾è©±ãƒ«ãƒ¼ãƒ—é–‹å§‹
"""

import logging
import os

os.environ["TORCHDYNAMO_DISABLE"] = "1"

import asyncio
import sys
from langchain_core.messages import HumanMessage, AIMessage

# EM-LLMé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from agent_core.em_llm_core import EMLLMIntegrator, EMConfig
from agent_core.em_llm_graph import EMEnabledAgentCore, EMCompatibilityLayer
from agent_core.embedding_provider import LlamaCppEmbeddingProvider 

# å¾“æ¥ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from agent_core.config import MCP_CONFIG_FILE, MAX_CHAT_HISTORY_LENGTH, EM_LLM_CONFIG
from agent_core.llm_manager import LLMManager
from agent_core.tool_manager import ToolManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def ainput(prompt: str = "") -> str:
    print(prompt, end="", flush=True)
    return await asyncio.to_thread(sys.stdin.readline)

async def main():
    """EM-LLMå¯¾å¿œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    llm_manager = None
    tool_manager = None
    em_llm_integrator = None
    app = None
    
    try:
        print("Initializing EM-LLM Enhanced AI Agent...")
        print("=" * 60)
        
        # === Phase 1: åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===
        print("Phase 1: Initializing core systems...")
        
        # LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        llm_manager = LLMManager()
        llm_manager.get_character_agent()  # ãƒ¡ã‚¤ãƒ³LLMã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
        print("âœ“ LLM Manager initialized")
        
        # ãƒ„ãƒ¼ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        tool_manager = ToolManager(config_file=MCP_CONFIG_FILE)
        tool_manager.initialize()
        print(f"âœ“ Tool Manager initialized with {len(tool_manager.tools)} tools")
        
        # === Phase 2: EM-LLM ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===
        print("\nPhase 2: Initializing EM-LLM systems...")
        
        try:
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            embedding_llm = llm_manager.get_embedding_model()
            embedding_provider = LlamaCppEmbeddingProvider(embedding_llm)
            print("âœ“ Embedding provider initialized")
            
            # EM-LLMçµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆæœŸåŒ–
            em_llm_integrator = EMLLMIntegrator(llm_manager, embedding_provider)
            print("âœ“ EM-LLM integrator initialized")
            
            # è¨­å®šå€¤ã‚’æ›´æ–°
            em_config = EMConfig(
                surprise_window=EM_LLM_CONFIG["surprise_window_size"],
                surprise_gamma=EM_LLM_CONFIG["surprise_gamma"],
                min_event_size=EM_LLM_CONFIG["min_event_size"],
                max_event_size=EM_LLM_CONFIG["max_event_size"],
                similarity_buffer_ratio=EM_LLM_CONFIG["similarity_buffer_ratio"],
                contiguity_buffer_ratio=EM_LLM_CONFIG["contiguity_buffer_ratio"],
                total_retrieved_events=EM_LLM_CONFIG["total_retrieved_events"],
                recency_weight=EM_LLM_CONFIG["recency_weight"],
                use_boundary_refinement=EM_LLM_CONFIG["use_boundary_refinement"],
                refinement_metric=EM_LLM_CONFIG["refinement_metric"],
                refinement_search_range=EM_LLM_CONFIG["refinement_search_range"]
            )
            em_llm_integrator.config = em_config
            print("âœ“ EM-LLM configuration applied")
            
        except Exception as e:
            logger.error(f"EM-LLM initialization failed: {e}")
            print(f"âš  EM-LLM initialization failed: {e}")
            print("Falling back to traditional memory system...")
            em_llm_integrator = None
        
        # === Phase 3: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•æ§‹ç¯‰ ===
        print("\nPhase 3: Building application graph...")
        
        if em_llm_integrator:
            # EM-LLMå¯¾å¿œã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
            agent_core = EMEnabledAgentCore(llm_manager, tool_manager, em_llm_integrator)
            app = agent_core.graph
            print("âœ“ EM-LLM enhanced graph initialized")
            
            # åˆæœŸçµ±è¨ˆã‚’è¡¨ç¤º
            try:
                stats = em_llm_integrator.get_memory_statistics()
                print(f"âœ“ EM-LLM Memory System: {stats['status'] if stats.get('total_events') == 0 else f'{stats['total_events']} events'}")
            except Exception as e:
                print(f"âš  Could not retrieve initial EM-LLM statistics: {e}")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ 
            from agent_core.memory.memory_system import MemorySystem
            memory_system = MemorySystem(embedding_provider)
            from agent_core.graph import AgentCore
            agent_core = AgentCore(llm_manager, tool_manager, memory_system)
            app = agent_core.graph
            print("âœ“ Traditional graph initialized (fallback mode)")
        
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"Critical error during initialization: {e}", exc_info=True)
        print(f"\nâŒ Failed to start the AI agent: {e}")
        print("Please check the logs and configuration.")
        return
    
    # === å¯¾è©±ãƒ«ãƒ¼ãƒ—é–‹å§‹ ===
    if em_llm_integrator:
        print("ğŸ§  EM-LLM Enhanced AI Agent is ready!")
        print("Features: Surprise-based memory formation, Two-stage retrieval, Episodic segmentation")
    else:
        print("ğŸ¤– AI Agent is ready (traditional mode)")
    
    print("\nCommands:")
    print("  â€¢ '/agentmode <request>' - Complex task with tools")  
    print("  â€¢ '/search <query>' - Web search")
    print("  â€¢ '/emstats' - EM-LLM memory statistics (if available)")
    print("  â€¢ Normal chat - Direct conversation")
    print("  â€¢ 'exit' - Quit")
    print("-" * 60)
    
    chat_history = []
    
    try:
        while True:
            try:
                user_input = (await ainput("You: ")).strip()
                
                if user_input.lower() in ["exit", "quit"]:
                    break
                if not user_input:
                    continue
                
                # EM-LLMçµ±è¨ˆã‚³ãƒãƒ³ãƒ‰å‡¦ç†
                if user_input.lower() == '/emstats' and em_llm_integrator:
                    try:
                        stats = em_llm_integrator.get_memory_statistics()
                        print("\nğŸ“Š EM-LLM Memory System Statistics:")
                        print(f"   Total Events: {stats.get('total_events', 0)}")
                        print(f"   Total Tokens: {stats.get('total_tokens_in_memory', 0)}")
                        print(f"   Mean Event Size: {stats.get('mean_event_size', 0):.1f} tokens")
                        print()
                        
                        surprise_stats = stats.get('surprise_statistics', {})
                        if surprise_stats and surprise_stats.get('mean', 0) > 0:
                            print(f"   Surprise - Mean: {surprise_stats.get('mean', 0):.3f}, "
                                  f"Std: {surprise_stats.get('std', 0):.3f}, Max: {surprise_stats.get('max', 0):.3f}")
                        
                        config_info = stats.get('configuration', {})
                        print(f"   Config - Î³: {config_info.get('surprise_gamma', 0)}, "
                              f"Event Size: {config_info.get('min_event_size', 0)}-{config_info.get('max_event_size', 0)}")
                        print()
                        continue
                    except Exception as e:
                        print(f"âŒ Failed to retrieve EM-LLM statistics: {e}")
                        continue
                
                # LangGraphã®å®Ÿè¡Œ
                initial_state = {
                    "input": user_input,
                    "chat_history": chat_history,
                    "agent_scratchpad": [],
                    "messages": [],
                }
                
                print(f"\n--- Processing (EM-LLM: {'âœ“' if em_llm_integrator else 'âœ—'}) ---")
                
                full_response = ""
                final_output = None
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
                async for event in app.astream_events(initial_state, version="v2", config={"recursion_limit": 50}):
                    kind = event["event"]
                    
                    # LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
                    if kind == "on_chat_model_stream":
                        content = event["data"]["chunk"].content
                        if content:
                            print(content, end="", flush=True)
                            full_response += content
                    
                    # ã‚°ãƒ©ãƒ•å®Ÿè¡Œå®Œäº†
                    elif kind == "on_graph_end":
                        final_output = event["data"]["output"]
                
                print()  # æ”¹è¡Œ
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´æ›´æ–°
                if full_response:
                    chat_history.append(HumanMessage(content=user_input))
                    chat_history.append(AIMessage(content=full_response))
                elif final_output and final_output.get("agent_outcome"):
                    print(f"\nAI: Task completed. Outcome: {final_output['agent_outcome']}")
                    chat_history.append(HumanMessage(content=user_input))
                else:
                    print("\nAI: An unexpected error occurred.")
                    chat_history.append(HumanMessage(content=user_input))
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®é•·ã•åˆ¶é™
                if len(chat_history) > MAX_CHAT_HISTORY_LENGTH:
                    chat_history = chat_history[-MAX_CHAT_HISTORY_LENGTH:]
                    print(f"INFO: Chat history truncated to {MAX_CHAT_HISTORY_LENGTH} messages.")
                
                print("-" * 60)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Exiting EM-LLM Agent. Goodbye!")
                break
            except Exception as e:
                logger.error(f"Error during conversation: {e}", exc_info=True)
                print(f"\nâŒ An error occurred: {e}")
                print("Please try again or type 'exit' to quit.")
    
    finally:
        # === ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===
        print("\nğŸ§¹ Cleaning up resources...")
        
        if llm_manager:
            try:
                llm_manager.cleanup()
                print("âœ“ LLM resources cleaned up")
            except Exception as e:
                print(f"âš  LLM cleanup warning: {e}")
        
        if tool_manager:
            try:
                tool_manager.cleanup()
                print("âœ“ Tool manager cleaned up")
            except Exception as e:
                print(f"âš  Tool manager cleanup warning: {e}")
        
        if em_llm_integrator:
            try:
                stats = em_llm_integrator.get_memory_statistics()
                print(f"âœ“ Final EM-LLM state: {stats.get('total_events', 0)} events, "
                      f"{stats.get('total_tokens_in_memory', 0)} tokens")
            except Exception as e:
                print(f"âš  Could not retrieve final EM-LLM statistics: {e}")
        
        print("Cleanup completed.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        logging.error(f"Failed to run the EM-LLM agent application: {e}", exc_info=True)
        print(f"âŒ Critical failure: {e}")
        print("Check logs for details.")